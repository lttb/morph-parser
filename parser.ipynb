{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.root.handlers = []  # Jupyter messes up logging so needs a reset\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n",
      "2017-03-30 20:37:50,462 : INFO : 'pattern' package not found; tag filters are not available for English\n",
      "/usr/local/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from smart_open import smart_open\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import nltk\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import linear_model\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-30 20:37:51,173 : INFO : loading projection weights from ../meetin/models/ruscorpora_russe.model.bin.gz\n",
      "2017-03-30 20:38:10,879 : INFO : loaded (374526, 300) matrix from ../meetin/models/ruscorpora_russe.model.bin.gz\n",
      "2017-03-30 20:38:10,879 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    }
   ],
   "source": [
    "wv = Word2Vec.load_word2vec_format(\n",
    "    '../meetin/models/ruscorpora_russe.model.bin.gz',\n",
    "    binary=True)\n",
    "wv.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.vocab:\n",
    "            mean.append(wv.syn0norm[wv.vocab[word].index])\n",
    "            all_words.add(wv.vocab[word].index)\n",
    "        else:\n",
    "            if word == '_WORD_':\n",
    "                mean.append(np.full((wv.layer1_size,), -1, dtype=float))\n",
    "\n",
    "            if word == '_0_':\n",
    "                mean.append(np.full((wv.layer1_size,), 0, dtype=float))\n",
    "\n",
    "            if word == '_NUM_':\n",
    "                mean.append(np.full((wv.layer1_size,), -3, dtype=float))\n",
    "    \n",
    "    if not mean:\n",
    "        return np.full((wv.layer1_size,), 0)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def  word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, word) for word in text_list ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-30 20:39:08,383 : INFO : Loading dictionaries from /usr/local/lib/python3.5/site-packages/pymorphy2_dicts/data\n",
      "2017-03-30 20:39:08,442 : INFO : format: 2.4, revision: 393442, updated: 2015-01-17T16:03:56.586168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/enload/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "import pymorphy2\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "\n",
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for i, word in enumerate(nltk.word_tokenize(sent)):\n",
    "            tokens.append(word if i > 0 else word.lower())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-30 20:39:10,145 : INFO : Loading dictionaries from /usr/local/lib/python3.5/site-packages/pymorphy2_dicts/data\n",
      "2017-03-30 20:39:10,202 : INFO : format: 2.4, revision: 393442, updated: 2015-01-17T16:03:56.586168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n']\n",
      "[{'context': 'и губой _WORD_ улыбочкой так', 'tags': ['A=f', 'sg', 'ins', 'plen']}, {'context': 'совершенно Правый _WORD_ устрашающий глаз', 'tags': ['A=sg', 'm', 'nom', 'plen']}, {'context': 'глаз один _WORD_ открылся и', 'tags': ['A=sg', 'm', 'nom', 'plen']}, {'context': 'светлой толстой _WORD_ и маленьким', 'tags': ['S', 'f', 'inan=sg', 'ins']}, {'context': 'с богатыря _WORD_ через спину', 'tags': ['S', 'f', 'inan=sg', 'ins']}]\n"
     ]
    }
   ],
   "source": [
    "import pymorphy2\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "with open('data/ruscorpora.parsed.txt', 'r') as myfile:\n",
    "    sentences = [s.split(';\\n') for s in myfile.read().split('_')]\n",
    "    \n",
    "    \n",
    "def get_word(s, index):\n",
    "    if index < 0 or index >= len(s):\n",
    "        return '_0_'\n",
    "    \n",
    "    word = s[index]\n",
    "\n",
    "    if word.isdigit():\n",
    "        word = '_NUM_'\n",
    "\n",
    "    return word.split('/')[0]\n",
    "    \n",
    "data_lex = {}\n",
    "for s in sentences:\n",
    "    index = 0\n",
    "    \n",
    "    for word in s:\n",
    "        parts = word.split('/')\n",
    "        \n",
    "        word = parts[0].lower()\n",
    "        \n",
    "        if len(parts) < 3:\n",
    "            print(parts)\n",
    "            continue\n",
    "            \n",
    "        tags = parts[2].replace(';', '').split(',')\n",
    "        \n",
    "        context = []\n",
    "        \n",
    "        for i in range(1, 3):\n",
    "            context += [get_word(s, index - i)]\n",
    "        \n",
    "        context += ['_WORD_']\n",
    "        \n",
    "        for i in range(1, 3):\n",
    "            context += [get_word(s, index + i)]\n",
    "            \n",
    "        context = ' '.join(context)\n",
    "        \n",
    "        index += 1\n",
    "        \n",
    "        if word not in data_lex:\n",
    "            data_lex[word] = []\n",
    "            \n",
    "        data_lex[word] += [{\n",
    "            'tags': tags,\n",
    "            'context': context\n",
    "        }]\n",
    "        \n",
    "print(data_lex['косой'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport json\\n\\ndata = json.loads(open('data/data_lex_final.json').read())\\n\\ndata_lex = {}\\n\\nfor word in data:\\n    [lemma] = word.keys()\\n    [params] = word.values()\\n    \\n    if lemma not in data_lex:\\n        data_lex[lemma] = []\\n    \\n    data_lex[lemma] += [params]\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import json\n",
    "\n",
    "data = json.loads(open('data/data_lex_final.json').read())\n",
    "\n",
    "data_lex = {}\n",
    "\n",
    "for word in data:\n",
    "    [lemma] = word.keys()\n",
    "    [params] = word.values()\n",
    "    \n",
    "    if lemma not in data_lex:\n",
    "        data_lex[lemma] = []\n",
    "    \n",
    "    data_lex[lemma] += [params]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_lex_vectors = {}\n",
    "\n",
    "for word, params in list(data_lex.items()):\n",
    "    contexts = [w2v_tokenize_text(x['context']) for x in params]\n",
    "    data_lex_vectors[word] = word_averaging_list(wv, contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\njust build all vectors, but we will build them lazy instead\\n\\nfor word, vectors in data_lex_vectors.items():\\n    vectors_trees[word] = spatial.KDTree(vectors)\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "vectors_trees = {}\n",
    "\n",
    "\n",
    "'''\n",
    "just build all vectors, but we will build them lazy instead\n",
    "\n",
    "for word, vectors in data_lex_vectors.items():\n",
    "    vectors_trees[word] = spatial.KDTree(vectors)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-30 21:01:05,271 : INFO : Loading dictionaries from /usr/local/lib/python3.5/site-packages/pymorphy2_dicts/data\n",
      "2017-03-30 21:01:05,352 : INFO : format: 2.4, revision: 393442, updated: 2015-01-17T16:03:56.586168\n"
     ]
    }
   ],
   "source": [
    "import pymorphy2\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "\n",
    "def match(word, context):\n",
    "    context_tokenized = w2v_tokenize_text(context)\n",
    "    context_normalized = [morph.normal_forms(w)[0] for w in context_tokenized]\n",
    "    \n",
    "    context_vector = word_averaging_list(wv, [context_tokenized])\n",
    "    \n",
    "    if word in vectors_trees:\n",
    "        tree = vectors_trees[word]\n",
    "    else:\n",
    "        tree = spatial.KDTree(data_lex_vectors[word])\n",
    "    \n",
    "    dist, index = tree.query(context_vector)\n",
    "    \n",
    "    return data_lex[word][index]['tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S', 'm', 'inan=sg', 'acc']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " match('берег', 'людей на _WORD_ _0_ _0_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S', 'm', 'inan=sg', 'loc2']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " match('берегу', 'я _0_ _WORD_ свои воспоминания')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-30 21:03:05,013 : INFO : Loading dictionaries from /usr/local/lib/python3.5/site-packages/pymorphy2_dicts/data\n",
      "2017-03-30 21:03:05,078 : INFO : format: 2.4, revision: 393442, updated: 2015-01-17T16:03:56.586168\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import analyzer\n",
    "\n",
    "def tokenize(text):\n",
    "    return [word.lower() for word in re.split(r'(?!\\b-\\b)\\W', text) if word]\n",
    "\n",
    "\n",
    "def desambiguate_sent(sent):\n",
    "    words = tokenize(sent)\n",
    "    ambig = []\n",
    "    poses = []\n",
    "    result = []\n",
    "    for word in words:\n",
    "        tags = morph.parse(word)[:3]\n",
    "        pos = []\n",
    "        for t in tags:\n",
    "            if t.tag.POS not in pos:\n",
    "                if t.tag.POS is None:\n",
    "                    pos.append('UNKN')\n",
    "                else:\n",
    "                    pos.append(t.tag.POS)\n",
    "        if len(pos) > 1:\n",
    "            ambig.append(True)\n",
    "            poses.append('/'.join(pos))\n",
    "        else:\n",
    "            poses.append(pos[0])\n",
    "            ambig.append(False)\n",
    "    for i in range(len(ambig)):\n",
    "        if ambig[i] is True:\n",
    "            context_lex = analyzer.make_context(words, i)\n",
    "            context_morph = analyzer.make_context(poses, i)\n",
    "            result.append([words[i], context_lex, poses[i], context_morph])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['стали',\n",
       "  '_0_ мы _WORD_ печь пирог',\n",
       "  'VERB/NOUN',\n",
       "  '_0_ NPRO _WORD_ INFN/NOUN NOUN'],\n",
       " ['печь',\n",
       "  'мы стали _WORD_ пирог _0_',\n",
       "  'INFN/NOUN',\n",
       "  'NPRO VERB/NOUN _WORD_ NOUN _0_']]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = desambiguate_sent('мы стали печь пирог')\n",
    "\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "стали - ['S', 'f', 'inan=sg', 'gen']\n",
      "печь - ['S', 'f', 'inan=sg', 'acc']\n"
     ]
    }
   ],
   "source": [
    "for s in sentences:\n",
    "    [word, context, *rest] = s\n",
    "    print(word, '-', match(word, context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
